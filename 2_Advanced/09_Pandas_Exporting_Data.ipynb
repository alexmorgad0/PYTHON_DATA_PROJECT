{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d27ba3e",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/lukebarousse/Python_Data_Analytics_Course/blob/main/2_Advanced/09_Pandas_Exporting_Data.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ah78BBuvJN8N"
   },
   "source": [
    "# Pandas Exporting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexm\\anaconda3\\envs\\python_course\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# Loading Data\n",
    "dataset = load_dataset('lukebarousse/data_jobs')\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Data Cleanup\n",
    "df['job_posted_date'] = pd.to_datetime(df['job_posted_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EGOzKqsJX3m"
   },
   "source": [
    "## Notes\n",
    "\n",
    "* `to_csv()`: Export DataFrame to CSV file. \n",
    "* `to_excel()`: Export DataFrame to Excel file.\n",
    "* `to_sql()`: Export DataFrame to SQL database.\n",
    "* `to_parquet()`: Export DataFrame to a parquet file. \n",
    "    * Parquet is a columnar storage file format that is designed for efficient data storage and retrieval.\n",
    "\n",
    "\n",
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's export our file to a **CSV file**. Often you may have to export files back and forth from dataframes to CSV files. Especially if you're trying to clean the data before using it in another data visualization tool like Tableau or PowerBI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame to a CSV file\n",
    "df.to_csv('jobs_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method to_csv in module pandas.core.generic:\n",
      "\n",
      "to_csv(path_or_buf: 'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None' = None, *, sep: 'str' = ',', na_rep: 'str' = '', float_format: 'str | Callable | None' = None, columns: 'Sequence[Hashable] | None' = None, header: 'bool_t | list[str]' = True, index: 'bool_t' = True, index_label: 'IndexLabel | None' = None, mode: 'str' = 'w', encoding: 'str | None' = None, compression: 'CompressionOptions' = 'infer', quoting: 'int | None' = None, quotechar: 'str' = '\"', lineterminator: 'str | None' = None, chunksize: 'int | None' = None, date_format: 'str | None' = None, doublequote: 'bool_t' = True, escapechar: 'str | None' = None, decimal: 'str' = '.', errors: 'OpenFileErrors' = 'strict', storage_options: 'StorageOptions | None' = None) -> 'str | None' method of pandas.core.frame.DataFrame instance\n",
      "    Write object to a comma-separated values (csv) file.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path_or_buf : str, path object, file-like object, or None, default None\n",
      "        String, path object (implementing os.PathLike[str]), or file-like\n",
      "        object implementing a write() function. If None, the result is\n",
      "        returned as a string. If a non-binary file object is passed, it should\n",
      "        be opened with `newline=''`, disabling universal newlines. If a binary\n",
      "        file object is passed, `mode` might need to contain a `'b'`.\n",
      "    sep : str, default ','\n",
      "        String of length 1. Field delimiter for the output file.\n",
      "    na_rep : str, default ''\n",
      "        Missing data representation.\n",
      "    float_format : str, Callable, default None\n",
      "        Format string for floating point numbers. If a Callable is given, it takes\n",
      "        precedence over other numeric formatting parameters, like decimal.\n",
      "    columns : sequence, optional\n",
      "        Columns to write.\n",
      "    header : bool or list of str, default True\n",
      "        Write out the column names. If a list of strings is given it is\n",
      "        assumed to be aliases for the column names.\n",
      "    index : bool, default True\n",
      "        Write row names (index).\n",
      "    index_label : str or sequence, or False, default None\n",
      "        Column label for index column(s) if desired. If None is given, and\n",
      "        `header` and `index` are True, then the index names are used. A\n",
      "        sequence should be given if the object uses MultiIndex. If\n",
      "        False do not print fields for index names. Use index_label=False\n",
      "        for easier importing in R.\n",
      "    mode : {'w', 'x', 'a'}, default 'w'\n",
      "        Forwarded to either `open(mode=)` or `fsspec.open(mode=)` to control\n",
      "        the file opening. Typical values include:\n",
      "    \n",
      "        - 'w', truncate the file first.\n",
      "        - 'x', exclusive creation, failing if the file already exists.\n",
      "        - 'a', append to the end of file if it exists.\n",
      "    \n",
      "    encoding : str, optional\n",
      "        A string representing the encoding to use in the output file,\n",
      "        defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n",
      "        is a non-binary file object.\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "        (otherwise no compression).\n",
      "        Set to ``None`` for no compression.\n",
      "        Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "        other key-value pairs are forwarded to\n",
      "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n",
      "        ``tarfile.TarFile``, respectively.\n",
      "        As an example, the following could be passed for faster compression and to create\n",
      "        a reproducible gzip archive:\n",
      "        ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      "    \n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "    \n",
      "           May be a dict with key 'method' as compression mode\n",
      "           and other entries as additional compression options if\n",
      "           compression mode is 'zip'.\n",
      "    \n",
      "           Passing compression options as keys in dict is\n",
      "           supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\n",
      "    quoting : optional constant from csv module\n",
      "        Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n",
      "        then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n",
      "        will treat them as non-numeric.\n",
      "    quotechar : str, default '\\\"'\n",
      "        String of length 1. Character used to quote fields.\n",
      "    lineterminator : str, optional\n",
      "        The newline character or character sequence to use in the output\n",
      "        file. Defaults to `os.linesep`, which depends on the OS in which\n",
      "        this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n",
      "    \n",
      "        .. versionchanged:: 1.5.0\n",
      "    \n",
      "            Previously was line_terminator, changed for consistency with\n",
      "            read_csv and the standard library 'csv' module.\n",
      "    \n",
      "    chunksize : int or None\n",
      "        Rows to write at a time.\n",
      "    date_format : str, default None\n",
      "        Format string for datetime objects.\n",
      "    doublequote : bool, default True\n",
      "        Control quoting of `quotechar` inside a field.\n",
      "    escapechar : str, default None\n",
      "        String of length 1. Character used to escape `sep` and `quotechar`\n",
      "        when appropriate.\n",
      "    decimal : str, default '.'\n",
      "        Character recognized as decimal separator. E.g. use ',' for\n",
      "        European data.\n",
      "    errors : str, default 'strict'\n",
      "        Specifies how encoding and decoding errors are to be handled.\n",
      "        See the errors argument for :func:`open` for a full list\n",
      "        of options.\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "        details, and for more examples on storage options refer `here\n",
      "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "        highlight=storage_options#reading-writing-remote-files>`_.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None or str\n",
      "        If path_or_buf is None, returns the resulting csv format as a\n",
      "        string. Otherwise returns None.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    read_csv : Load a CSV file into a DataFrame.\n",
      "    to_excel : Write DataFrame to an Excel file.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Create 'out.csv' containing 'df' without indices\n",
      "    \n",
      "    >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
      "    ...                    'mask': ['red', 'purple'],\n",
      "    ...                    'weapon': ['sai', 'bo staff']})\n",
      "    >>> df.to_csv('out.csv', index=False)  # doctest: +SKIP\n",
      "    \n",
      "    Create 'out.zip' containing 'out.csv'\n",
      "    \n",
      "    >>> df.to_csv(index=False)\n",
      "    'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n",
      "    >>> compression_opts = dict(method='zip',\n",
      "    ...                         archive_name='out.csv')  # doctest: +SKIP\n",
      "    >>> df.to_csv('out.zip', index=False,\n",
      "    ...           compression=compression_opts)  # doctest: +SKIP\n",
      "    \n",
      "    To write a csv file to a new folder or nested folder you will first\n",
      "    need to create it using either Pathlib or os:\n",
      "    \n",
      "    >>> from pathlib import Path  # doctest: +SKIP\n",
      "    >>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      "    >>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n",
      "    >>> df.to_csv(filepath)  # doctest: +SKIP\n",
      "    \n",
      "    >>> import os  # doctest: +SKIP\n",
      "    >>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n",
      "    >>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.to_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular form to export to for your non code savy friends is to Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Saving the DataFrame to an Excel file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjobs_data.xlsx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexm\\anaconda3\\envs\\python_course\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexm\\anaconda3\\envs\\python_course\\Lib\\site-packages\\pandas\\core\\generic.py:2417\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2406\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2407\u001b[39m     df,\n\u001b[32m   2408\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2415\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2416\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2417\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexm\\anaconda3\\envs\\python_course\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexm\\anaconda3\\envs\\python_course\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:57\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     path: FilePath | WriteExcelBuffer | ExcelWriter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m     engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m         path,\n\u001b[32m     63\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m     67\u001b[39m     )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "# Saving the DataFrame to an Excel file\n",
    "df.to_excel('jobs_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, is writing to a **SQL database**. This will only work if you actually have a database to write to. In our case we don't, so this is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the DataFrame to a SQL database\n",
    "\n",
    "# this requires a connection to a SQL database, we'll use sqlalchemy for this\n",
    "# !conda install -c anaconda sqlalchemy -y\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///jobs.db')\n",
    "\n",
    "df.to_sql('job_table', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also able to export a Dataframe to a **parquet file** or **pickle file**. We won't be going into that during the video, but it's good to be aware of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame to a Parquet file\n",
    "df.to_parquet('jobs_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame to a Pickle file\n",
    "df.to_pickle('job_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export our DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the DataFrame we created in the last section from our actual job postings DataFrame and export it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title_short</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_via</th>\n",
       "      <th>job_schedule_type</th>\n",
       "      <th>job_work_from_home</th>\n",
       "      <th>search_location</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_no_degree_mention</th>\n",
       "      <th>job_health_insurance</th>\n",
       "      <th>job_country</th>\n",
       "      <th>salary_rate</th>\n",
       "      <th>salary_year_avg</th>\n",
       "      <th>salary_hour_avg</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>job_type_skills</th>\n",
       "      <th>job_posted_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Associate Data Analyst- Customer Experience Group</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>via Trabajo.org</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>2023-01-07 07:25:49</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agoda</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analyst (actuarial)</td>\n",
       "      <td>Frederick, MD</td>\n",
       "      <td>via Lensa</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>New York, United States</td>\n",
       "      <td>2023-01-17 07:00:25</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BVO Recruiters</td>\n",
       "      <td>['r', 'python', 'sql', 'spreadsheet', 'excel',...</td>\n",
       "      <td>{'analyst_tools': ['spreadsheet', 'excel', 'ta...</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Governance Data Analyst</td>\n",
       "      <td>Cebu City, Cebu, Philippines</td>\n",
       "      <td>via Trabajo.org</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>2023-01-03 17:00:02</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lexmark</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analyst III</td>\n",
       "      <td>Fort Sam Houston, TX</td>\n",
       "      <td>via JobServe</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>Texas, United States</td>\n",
       "      <td>2023-01-25 07:01:36</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Superior HealthPlan</td>\n",
       "      <td>['sql', 'excel', 'sap']</td>\n",
       "      <td>{'analyst_tools': ['excel', 'sap'], 'programmi...</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Manager, Data Analytics</td>\n",
       "      <td>Johannesburg, South Africa</td>\n",
       "      <td>via BeBee South Africa</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>2023-01-21 07:33:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Standard Bank Of South Africa Limited</td>\n",
       "      <td>['sql', 'powerpoint', 'excel', 'word', 'outloo...</td>\n",
       "      <td>{'analyst_tools': ['powerpoint', 'excel', 'wor...</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56537</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data &amp; Analytics Architect (w/m/x)</td>\n",
       "      <td>Erfurt, Jerman</td>\n",
       "      <td>melalui LinkedIn</td>\n",
       "      <td>Pekerjaan tetap</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2023-03-12 06:18:18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NTT DATA DACH</td>\n",
       "      <td>['aws', 'azure']</td>\n",
       "      <td>{'cloud': ['aws', 'azure']}</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56538</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Amul Careers 2023 - Apply Online - Data Analys...</td>\n",
       "      <td>India</td>\n",
       "      <td>melalui Jobsleworld - Jobs In India - Job Vaca...</td>\n",
       "      <td>Pekerjaan tetap</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>2023-03-13 06:16:28</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amul</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56539</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analyst (m/w/d)</td>\n",
       "      <td>Jena, Jerman</td>\n",
       "      <td>melalui XING</td>\n",
       "      <td>Pekerjaan tetap</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2023-03-12 06:18:18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linimed Gruppe</td>\n",
       "      <td>['sql', 'julia', 'power bi', 'dax']</td>\n",
       "      <td>{'analyst_tools': ['power bi', 'dax'], 'progra...</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56540</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Research Analyst</td>\n",
       "      <td>India</td>\n",
       "      <td>melalui BeBee India</td>\n",
       "      <td>Pekerjaan tetap</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>2023-03-13 06:16:28</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>India</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zap Recruit</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56541</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Lead Data Analyst</td>\n",
       "      <td>Jerman</td>\n",
       "      <td>melalui BeBee Deutschland</td>\n",
       "      <td>Pekerjaan tetap</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2023-03-12 06:18:18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Germany</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amtrak</td>\n",
       "      <td>['vba', 'sql', 'python', 'excel', 'sap', 'shar...</td>\n",
       "      <td>{'analyst_tools': ['excel', 'sap', 'sharepoint...</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56542 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      job_title_short                                          job_title  \\\n",
       "0        Data Analyst  Associate Data Analyst- Customer Experience Group   \n",
       "1        Data Analyst                           Data Analyst (actuarial)   \n",
       "2        Data Analyst                       Data Governance Data Analyst   \n",
       "3        Data Analyst                                   Data Analyst III   \n",
       "4        Data Analyst                            Manager, Data Analytics   \n",
       "...               ...                                                ...   \n",
       "56537    Data Analyst                 Data & Analytics Architect (w/m/x)   \n",
       "56538    Data Analyst  Amul Careers 2023 - Apply Online - Data Analys...   \n",
       "56539    Data Analyst                               Data Analyst (m/w/d)   \n",
       "56540    Data Analyst                              Data Research Analyst   \n",
       "56541    Data Analyst                                  Lead Data Analyst   \n",
       "\n",
       "                       job_location  \\\n",
       "0                            Jordan   \n",
       "1                     Frederick, MD   \n",
       "2      Cebu City, Cebu, Philippines   \n",
       "3              Fort Sam Houston, TX   \n",
       "4        Johannesburg, South Africa   \n",
       "...                             ...   \n",
       "56537                Erfurt, Jerman   \n",
       "56538                         India   \n",
       "56539                  Jena, Jerman   \n",
       "56540                         India   \n",
       "56541                        Jerman   \n",
       "\n",
       "                                                 job_via job_schedule_type  \\\n",
       "0                                        via Trabajo.org         Full-time   \n",
       "1                                              via Lensa         Full-time   \n",
       "2                                        via Trabajo.org         Full-time   \n",
       "3                                           via JobServe         Full-time   \n",
       "4                                 via BeBee South Africa         Full-time   \n",
       "...                                                  ...               ...   \n",
       "56537                                   melalui LinkedIn   Pekerjaan tetap   \n",
       "56538  melalui Jobsleworld - Jobs In India - Job Vaca...   Pekerjaan tetap   \n",
       "56539                                       melalui XING   Pekerjaan tetap   \n",
       "56540                                melalui BeBee India   Pekerjaan tetap   \n",
       "56541                          melalui BeBee Deutschland   Pekerjaan tetap   \n",
       "\n",
       "       job_work_from_home          search_location     job_posted_date  \\\n",
       "0                   False                   Jordan 2023-01-07 07:25:49   \n",
       "1                   False  New York, United States 2023-01-17 07:00:25   \n",
       "2                   False              Philippines 2023-01-03 17:00:02   \n",
       "3                   False     Texas, United States 2023-01-25 07:01:36   \n",
       "4                   False             South Africa 2023-01-21 07:33:00   \n",
       "...                   ...                      ...                 ...   \n",
       "56537               False                  Germany 2023-03-12 06:18:18   \n",
       "56538               False                    India 2023-03-13 06:16:28   \n",
       "56539               False                  Germany 2023-03-12 06:18:18   \n",
       "56540               False                    India 2023-03-13 06:16:28   \n",
       "56541               False                  Germany 2023-03-12 06:18:18   \n",
       "\n",
       "       job_no_degree_mention  job_health_insurance    job_country salary_rate  \\\n",
       "0                       True                 False         Jordan        None   \n",
       "1                      False                 False  United States        None   \n",
       "2                      False                 False    Philippines        None   \n",
       "3                      False                  True  United States        None   \n",
       "4                       True                 False   South Africa        None   \n",
       "...                      ...                   ...            ...         ...   \n",
       "56537                  False                 False        Germany        None   \n",
       "56538                  False                 False          India        None   \n",
       "56539                  False                 False        Germany        None   \n",
       "56540                  False                 False          India        None   \n",
       "56541                  False                 False        Germany        None   \n",
       "\n",
       "       salary_year_avg  salary_hour_avg  \\\n",
       "0                  NaN              NaN   \n",
       "1                  NaN              NaN   \n",
       "2                  NaN              NaN   \n",
       "3                  NaN              NaN   \n",
       "4                  NaN              NaN   \n",
       "...                ...              ...   \n",
       "56537              NaN              NaN   \n",
       "56538              NaN              NaN   \n",
       "56539              NaN              NaN   \n",
       "56540              NaN              NaN   \n",
       "56541              NaN              NaN   \n",
       "\n",
       "                                company_name  \\\n",
       "0                                      Agoda   \n",
       "1                             BVO Recruiters   \n",
       "2                                    Lexmark   \n",
       "3                        Superior HealthPlan   \n",
       "4      Standard Bank Of South Africa Limited   \n",
       "...                                      ...   \n",
       "56537                          NTT DATA DACH   \n",
       "56538                                   Amul   \n",
       "56539                         linimed Gruppe   \n",
       "56540                            Zap Recruit   \n",
       "56541                                 Amtrak   \n",
       "\n",
       "                                              job_skills  \\\n",
       "0                                                   None   \n",
       "1      ['r', 'python', 'sql', 'spreadsheet', 'excel',...   \n",
       "2                                                   None   \n",
       "3                                ['sql', 'excel', 'sap']   \n",
       "4      ['sql', 'powerpoint', 'excel', 'word', 'outloo...   \n",
       "...                                                  ...   \n",
       "56537                                   ['aws', 'azure']   \n",
       "56538                                               None   \n",
       "56539                ['sql', 'julia', 'power bi', 'dax']   \n",
       "56540                                               None   \n",
       "56541  ['vba', 'sql', 'python', 'excel', 'sap', 'shar...   \n",
       "\n",
       "                                         job_type_skills job_posted_month  \n",
       "0                                                   None              Jan  \n",
       "1      {'analyst_tools': ['spreadsheet', 'excel', 'ta...              Jan  \n",
       "2                                                   None              Jan  \n",
       "3      {'analyst_tools': ['excel', 'sap'], 'programmi...              Jan  \n",
       "4      {'analyst_tools': ['powerpoint', 'excel', 'wor...              Jan  \n",
       "...                                                  ...              ...  \n",
       "56537                        {'cloud': ['aws', 'azure']}              Mar  \n",
       "56538                                               None              Mar  \n",
       "56539  {'analyst_tools': ['power bi', 'dax'], 'progra...              Mar  \n",
       "56540                                               None              Mar  \n",
       "56541  {'analyst_tools': ['excel', 'sap', 'sharepoint...              Mar  \n",
       "\n",
       "[56542 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_DA = df[(df['job_title_short'] == 'Data Analyst')].copy()\n",
    "df_DA['job_posted_month'] = df_DA['job_posted_date'].dt.strftime('%b')\n",
    "\n",
    "months = df_DA['job_posted_month'].unique()\n",
    "df_DA_month = {}\n",
    "for month in months:\n",
    "    df_DA_month[month] = df_DA[df_DA['job_posted_month'] == month].copy()\n",
    "\n",
    "quarter_1 = [df_DA_month['Jan'], df_DA_month['Feb'], df_DA_month['Mar']]\n",
    "df_concat = pd.concat(quarter_1, ignore_index=True)\n",
    "\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our jobs to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame to a CSV file\n",
    "df_concat.to_csv('jobs_1st_quarter'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO7QGE4EmxtJp+NvT7AZdbk",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
